---
title: "08 Resumen Arima"
author: "Eddie Aguilar"
format:
  html:
    self-contained: true
editor: source
---

```{r}
#| message: false
library(easypackages)
libraries("tidyverse","fpp3", "patchwork")
```

ARIMA es otro modelo de pronóstico, a diferencia de ETS que se enfoca en la forma de la tendencia y estacionalidad, los modelos ARIMA tratan describir las autocorrelaciones en los datos. 

# Estacionariedad y diferenciación

Una serie de tiempo estacionaria (no confundir con estacionalidad) es aquella en la cual sus propiedades no dependen del tiempo en que son medidas. Por lo tanto, una serie con tendencia y/o estacionalidad no es estacionaria. 

Un comportamiento cíclico sé es estacionario porque no depende de un periodo de tiempo fijo.

O sea, no tendrá patrones visibles a lo largo del palzo. 

Una gráfica estacionaria mostrará una serie horizontal y con varianza constante.

Ejemplos:

![](Imagenes/series_est.png){width=85%}
- a y d tienen tendencia, por lo tanto no es estacionaria
- c y f tienen estacionalidad, por lo tanto no es estacionaria
- b y e son estacionarias (e parece tener estacionalidad, pero en realidad es un ciclo porque no es un periodo de tiempo definido)


## Diferenciación 

Como podemos ver en el ejemplo anterior, a) Precio de acción de Google tiene tendencia y no es estacionaria, sin embargo, b) Cambios diarios en el precio de acción de Google sí es estacionario. 

Esto nos deja ver, que **una manera de convertir una serie en estacionaria es calculando las diferencias entre observaciones consecutivas**, esto se le llama diferenciar la serie:

$$y'_t = y_t - y_{t-1}$$

La primera diferencia tendrá T - 1 observaciones y así sucesivamente. 

- Las transformaciones logarítmicas pueden ayudar a estabilizar la varianza de una serie. 
- Igualmente, la diferenciación puede ayudar a estabilizar la media de una serie de tiempo, al quitar los cambioes de nivel en ella y eliminar tendencia o estacionalidad. 


Podemos ver si una serie es estacionaria con ACF, si es estacionaria los valores se vuelven rapidamente cero en la función, de otra forma, decae lentamente y el valor del primer rezago es muy alto. 

```{r}
google_2015 <- gafa_stock %>% filter(Symbol == "GOOG" & year(Date) == 2015)

goog <- google_2015 %>% mutate(diff_close = Close)
for (i in 2:nrow(goog)){
  goog[i, length(goog)] = goog[i, 6] - goog[i-1, 6]
}
goog[1, length(goog)] = 0
goog
goog %>% ACF(Close) %>% autoplot() | 
  goog %>% ACF(diff_close) %>% autoplot()
```

Confirmando probando Ljung-Box o Box-Pierce:

```{r}
goog %>%
  features(diff_close, ljung_box, lag = 10)
```
Las autocorrelaciones no son significativas, son ruido blanco. 


## Diferenciación de segundo orden 

A veces la primera diferenciación no es suficiente y parece seguir siendo estacionaria, por lo tanto, se utiliza una segunda diferencia, normalmente en la práctica no pasaremos de la segunda diferencia. 

$$y''_t = y'_t - y'_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}$$
La serie en segundas diferencias tendrá T - 2 observaciones, esta representa los cambios en los cambios de la serie. 


## Diferenciación estacional 
(lag-m differences)

Es la diferencia que entre una observación y la observació previa de la misma estación: 

$$y'_t = y_t - t_{t-m}$$

donde m es el número de estaciones. 

Hay veces que es necesario tomar diferencias estacionales y primeras diferencias para lograr que la serie se convierta en estacionaria. 

```{r}
PBS
PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost)/1e6) %>%
  transmute(
    `Sales ($million)` = Cost,
    `Log sales` = log(Cost),
    `Annual change in log sales` = difference(log(Cost), 12),
    `Doubly differenced log sales` = difference(difference(log(Cost), 12), 1)
  ) %>%
  gather("Type", "Sales", !!!syms(measured_vars(.)), factor_key = TRUE) %>%
  ggplot(aes(x = Month, y = Sales)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y") +
  labs(title = "Corticosteroid drug sales", x = "Year", y = NULL)
```

**El orden en que se realiza la diferenciación no afecta el resultado**, hay casos que solo es necesario la diferenciación estacional para convertirla en estacionaria. 


## Pruebas de raíz unitaria

Pruebas formales para determinar si una serie es estacionaria o no. 

Existen muchas pruebas distintas de raíz unitaria. Utilizaremos, por lo pronto la prueba propuesta por Kwiatkowski-Phillips-Schmidt-Shin, o prueba KPSS en corto. Aquí, la H0 es que la serie es estacionaria. Por lo tanto, un p-value alto indicará que sí es estacionaria, mientras que un p-value < $\alpha$ indicará que la serie no es estacionaria.

Con el precio de la acción de Google: (no es estacionaria)

```{r}
features(goog, Close, unitroot_kpss)
```
Con las primeras diferencias: (sí es)

```{r}
goog %>%
  features(diff_close, unitroot_kpss)
```
**unitroot_ndiffs() nos dice el orden de diferenciación necesario para convertir la serie en estacionaria**

```{r}
features(goog, Close, unitroot_ndiffs)
```
Necesita solo una diferenciación


**Lo mismo con unitroot_nsdiffs() para diferenciación estacional**

```{r}
aus_total_retail <- aus_retail %>%
  summarise(Turnover = sum(Turnover))
autoplot(aus_total_retail)
```

Vemos que no es estacionaria, tiene tendencia y estacionalidad. 

Para estabilizar la varianza, usamos transformación logarítmica: 

```{r}
aus_total_retail <- aus_total_retail %>%
  mutate(log_turnover = log(Turnover)) 

aus_total_retail %>% autoplot(log_turnover)
```

Viendo el orden de diferenciación estacional: 

```{r}
aus_total_retail %>%
  features(log_turnover, unitroot_nsdiffs)
```

Viendo el orden de diferenciación: 

```{r}
aus_total_retail %>%
  mutate(log_turnover = difference(log(Turnover), 12)) %>%
  features(log_turnover, unitroot_ndiffs)
```

Se requieren ambas. 


## Notación de rezagos y diferencias 


El operador $B$ nos puede servir para representar rezagos en la serie de tiempo: 
$$By_t = y_{t-1}$$

Dos periodos atrás: 
$$B^{2}y_t = y_{t-1}$$

Ej, para datos mensuales, el mismo mes del año anterior: 
$$B^{12}y_t = y_{t-12}$$


# Modelos AR y MA

## Modelos autorregresivos (AR)

Modelo autorregresivo de orden $p$, AR($p$): 

$$ y_t = \phi_0 + \phi_1y_{t-1} + \phi_2y_{t-2} + ... + \phi_py_{t-p} + e_t$$
donde $e_t$ es ruido blanco. La ecuacióin es muy similar a una regresión lineal múltiple, pero los parámetros no son $\beta$ y las variables x ahora son valores rezagados de la variable dependiente. 

Con $y_t = \phi_0 + \phi_1y_{t-1} + e_t$, situaciones: 

- Si $\phi_1$ = 0, la serie es un ruido blanco.
- Si $\phi_1$ = 1 y $\phi_0$ = 0, la serie es una caminata aleatoria. 
- Si $\phi_1$ = 1 y $\phi_0$ != 0, la serie es una caminata aleatoria con deriva. 
- Si $\phi_1$ < 0, $y_t$ tiende a oscilar alrededor de su media. 


## Modelos de media móvil (MA)

A diferencia de los modelos autorregresivos que utilizan las observaciones pasadas, los de media móvil utilizan los errores pasados para modelar el pronóstico. 
$$y_t = \theta_0 + e_t + \theta_1e_{t-1} + \theta_2e_{t-2} + ... + \theta_qe_{t-q} $$

# Modelos ARIMA no estacionales

Si juntamos diferenciación, modelos autorregresivos y de media móvil, obtenemos un modelo ARIMA no estacional. (AutoRegressive Integrated Moving Average)

$$ y_t = \phi_0 + \phi_1y_{t-1} + \phi_2y_{t-2} + ... + \phi_py_{t-p} + e_t$$
$$y'_t = c + \phi_1y'_{t-1} + ... + \phi_py'_{t-p} + \theta_1e_{t-1} + ... + \theta_qe_{t-q} + e_t$$

Al tener la serie diferenciada, valores rezagados y errores rezagados. Se tiene que: 
**ARIMA(p,d,q)**, donde:

- p = orden del componente autorregresivo
- d = orden de diferenciación de la serie para hacerla estacionaria
- q = orden del componente de media móvil

Varios modelos hasta ahorita: 

![](Imagenes/arima_modelos.png){width=85%}

Efecto de los valores de la constate c y el orden de integración, d: 

![](Imagenes/arima_efectos.png){width=85%}


El valor de d incrementará los intervalos de predicción más rapidamente entre más grande sea su valor. 


## Función de autocorrelacióni y autocorrelación parcial 

Para saber el orden p y q podemos utilizar las funciones ACF y PACF. 

Los datos pueden ser ARIMA(p, d, 0) si:
- La ACF decae exponencialmente o tiene un comportamiento senodial.
- La PACF tiene un pico sifnificativo en rezago p, y posteriormente ya tienden a cero. 

Los datos pueden ser ARIMA(0, d, q) si: 
- La PACF decae exponencialmente o tiene un comportamiento senodial.
- La ACF tiene un pico sifnificativo en rezago q, y posteriormente ya tienden a cero. 

O sea, la PACF nos sirve para encontrar el orden p y ACF para el orden q.

```{r}
# us_change <- read_csv("us_change.csv") %>%
#   mutate(Time = yearquarter(Time)) %>%
#   as_tsibble(index = Time)

us_change %>% autoplot(Consumption) +
  labs(x = "Year", y = "Quarterly percentage change", title = "US consumption")
```

```{r}
us_change %>% PACF(Consumption) %>% autoplot()
```
Al tener un pico significativo y posteriormente ser 0, p sí se usara, al ser los primeros 3 rezagos significativos, esto nos deja ver que p = 3, ARIMA(3, 0, 0).

ARIMA() nos permite ajustar este tipo de modelos y con report() podemos ver el resultado del ajuste. El orden del modelo se especifica dentro de pdq(). para no estacionales también se pone PQD(0, 0,0)

```{r}
fit <- us_change %>%
  model(ARIMA(Consumption ~ pdq(3,0,0) + PDQ(0,0,0)))
report(fit)
```

**Si no especificamos el orden, R encontrará la mejor opción, sin embargo, a veces no logra encontrar el óptimo y es una operación tardada**

```{r}
fit2 <- us_change %>%
  model(ARIMA(Consumption ~ PDQ(0,0,0)))
report(fit2)
```
Encontró un ARIMA(1,0,3)

## Selección de modelos ARIMA

Para saber cual es el mejor modelo entre dos, podemos usar los criterios de información:

- Criterio de información de Akaike (AIC).
- Cirterio de información de Akaike corregido por sesgo (AICc)
- Criterio de información Batesiano (BIC).

Sobre todo se puede utilizar AICc, sabiendo que tener menor sea, mejor ajuste tendrá el modelo. 

Viendo el report de los modelos anteriores, ARIMA(1,0,3) tiene un AICc = 342.08 y ARIMA(3,0,0) tiene un AICc = 340.67, por lo tanto ARIMA(3,0,0) es mejor modelo. 

Para tener una evaluación automática más robusta pero más lenta, podemos usar stepwise=FALSE y approximation=FALSE. 

```{r}
fit3 <- us_change %>%
  model(ARIMA(Consumption ~ PDQ(0,0,0),
              stepwise = FALSE, approximation = FALSE))
report(fit3)
```
Vemos que ya nos da (3,0,0)

Igualmente podemos poner rango de valores, por ejemplo para p entre 1 y 3, para q entre 0 y 2:

```{r}
fit4 <- us_change %>%
  model(ARIMA(Consumption ~ pdq(1:3, 0, 0:2) + PDQ(0,0,0)))
report(fit4)
```

# Metodología Box-Jenkins

Proceso iterativo para encontrar un modelo (p, d, q):

1. Graficar los datos e identificar observaciones inusuales.
2. Si es necesario, transformar los datos para estabilizar la varianza. (Box-Cox casi siempre)
3. Si la serie es no estacionaria, diferenciarla hasta convertirla en estacionaria. 
4. Revisar las gráficas de las funciones ACF y PACF y decidir los ordenes p,q.
5. Ajustar el modelo escogido y revise la AICc correspondiente para comparar con otros modelos. 
6. Llevar a cabo el diagnóstico de residuos. 
7. Ya con residuos similares a ruido blanco, realizar los pronósticos. 

## Ejemplo

```{r}
elec_equip <- as_tsibble(fpp2::elecequip)

elec_dcmp <- elec_equip %>%
  model(STL(value ~ season(window="periodic"))) %>%
  components() %>%
  select(-.model) %>%
  as_tsibble()
elec_dcmp %>%
  autoplot(season_adjust)
```
1. Se muestra un cambio drástico de 2008 a 2009 por la crisis de esos años. 

2. No parece existir un cambio en la varianza, no es necesario una transformación. 

3. A pesar de no tener estacionalidad, todavía no es estacionaria, hay que aplicar primera diferencia. 

```{r}
elec_dcmp %>%
  gg_tsdisplay(difference(season_adjust), plot_type='partial')
```

4. La ACF decae exponencialmente y PACF tiene un pico y luego se va a cero, por lo tanto se usara (p,d, 0), en este caso, al tener 3 datos como pico en PACF, se escoge: ARIMA(3, 1, 0).

5. Se ajusta este modelo junto con otros de comparación, viendo AICc, ARIMA(3, 1, 1) es la mejor:

```{r}
fit <- elec_dcmp %>%
  model(
    arima310 = ARIMA(season_adjust ~ pdq(3,1,0) + PDQ(0,0,0)),
    arima410 = ARIMA(season_adjust ~ pdq(4,1,0) + PDQ(0,0,0)),
    arima210 = ARIMA(season_adjust ~ pdq(2,1,0) + PDQ(0,0,0)),
    arima311 = ARIMA(season_adjust ~ pdq(3,1,1) + PDQ(0,0,0))
    
  )

glance(fit)
```

6. La función ACF de los residuos nos deja ver que sí es ruido blanco. 

```{r}
fit %>% select(arima311) %>% gg_tsresiduals()
```
Ljung-Box lo confirma, el p value es muy alto: 
```{r}
fit %>% 
  select(arima311) %>% 
  augment() %>%
  features(.resid, ljung_box, lag = 24, dof = 4)
```
7. Pronóstico del modelo ARIMA(3, 1, 1):

```{r}
fit %>% select(arima311) %>% forecast() %>% autoplot(elec_dcmp)
```

# Modelos ARIMA estacionales (SARIMA)

Además de (p,d,q), se agrega (P,D,Q)m para estimar la estacionalidad también.


## Función de autocorrelación y autocorrelación parcial

De la misma forma, si ACF y PACF muestran los mismos rezagos que en ARIMA normal pero en los periodos m para obtener los mejores datos de P,D,Q.

